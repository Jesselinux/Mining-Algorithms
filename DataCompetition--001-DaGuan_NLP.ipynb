{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>word_seg</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7368 1252069 365865 755561 1044285 129532 1053...</td>\n",
       "      <td>816903 597526 520477 1179558 1033823 758724 63...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>581131 165432 7368 957317 1197553 570900 33659...</td>\n",
       "      <td>90540 816903 441039 816903 569138 816903 10343...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7368 87936 40494 490286 856005 641588 145611 1...</td>\n",
       "      <td>816903 1012629 957974 1033823 328210 947200 65...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>299237 760651 299237 887082 159592 556634 7489...</td>\n",
       "      <td>563568 1239563 680125 780219 782805 1033823 19...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7368 7368 7368 865510 7368 396966 995243 37685...</td>\n",
       "      <td>816903 816903 816903 139132 816903 312320 1103...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>7368 1160791 299237 1238054 569999 1044285 117...</td>\n",
       "      <td>816903 669476 21577 520477 1004165 4184 616471...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>893673 7368 836872 674898 231468 856005 105964...</td>\n",
       "      <td>277781 816903 1098157 986174 1033823 780491 10...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1122654 125310 907560 1172361 979583 983951 12...</td>\n",
       "      <td>289186 640942 363388 585102 261174 1217680 520...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>793790 599682 1223643 1030656 569999 178976 45...</td>\n",
       "      <td>1257015 966562 1054308 599826 811205 520477 28...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>7368 1120647 360394 79747 1140778 472252 7368 ...</td>\n",
       "      <td>816903 266069 1226448 1276450 816903 769051 12...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            article  \\\n",
       "0   0  7368 1252069 365865 755561 1044285 129532 1053...   \n",
       "1   1  581131 165432 7368 957317 1197553 570900 33659...   \n",
       "2   2  7368 87936 40494 490286 856005 641588 145611 1...   \n",
       "3   3  299237 760651 299237 887082 159592 556634 7489...   \n",
       "4   4  7368 7368 7368 865510 7368 396966 995243 37685...   \n",
       "5   5  7368 1160791 299237 1238054 569999 1044285 117...   \n",
       "6   6  893673 7368 836872 674898 231468 856005 105964...   \n",
       "7   7  1122654 125310 907560 1172361 979583 983951 12...   \n",
       "8   8  793790 599682 1223643 1030656 569999 178976 45...   \n",
       "9   9  7368 1120647 360394 79747 1140778 472252 7368 ...   \n",
       "\n",
       "                                            word_seg  class  \n",
       "0  816903 597526 520477 1179558 1033823 758724 63...     14  \n",
       "1  90540 816903 441039 816903 569138 816903 10343...      3  \n",
       "2  816903 1012629 957974 1033823 328210 947200 65...     12  \n",
       "3  563568 1239563 680125 780219 782805 1033823 19...     13  \n",
       "4  816903 816903 816903 139132 816903 312320 1103...     12  \n",
       "5  816903 669476 21577 520477 1004165 4184 616471...     13  \n",
       "6  277781 816903 1098157 986174 1033823 780491 10...      1  \n",
       "7  289186 640942 363388 585102 261174 1217680 520...     10  \n",
       "8  1257015 966562 1054308 599826 811205 520477 28...     10  \n",
       "9  816903 266069 1226448 1276450 816903 769051 12...     19  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一部分：\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 读取数据\n",
    "df_train = pd.read_csv('../DataSets/DaGuan/train_set.csv')    # 若内存不足可添加参数:nrows=1000\n",
    "df_test = pd.read_csv('../DataSets/DaGuan//test_set.csv')\n",
    "\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 数据预处理\n",
      "2 特征工程\n",
      "3 保存至本地\n",
      "已将原始数据数字化为tfidf特征，共耗时：9.539328424135844min\n"
     ]
    }
   ],
   "source": [
    "#第二部分：将原始数据数字化为tf-idf特征，并将结果保存至本地\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "\n",
    "#1 数据预处理\n",
    "\n",
    "print(\"1 数据预处理\")\n",
    "df_train.drop(columns='article', inplace=True)\n",
    "df_test.drop(columns='article', inplace=True)\n",
    "\n",
    "f_all = pd.concat(objs=[df_train, df_test], axis=0, sort=True)\n",
    "y_train = (df_train['class'] - 1).values\n",
    "\n",
    "\n",
    "#2 特征工程\n",
    "\n",
    "print(\"2 特征工程\")\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, sublinear_tf=True)\n",
    "vectorizer.fit(df_train['word_seg'])\n",
    "x_train = vectorizer.transform(df_train['word_seg'])\n",
    "x_test = vectorizer.transform(df_test['word_seg'])\n",
    "\n",
    "\n",
    "#3 保存至本地\n",
    "\n",
    "print(\"3 保存至本地\")\n",
    "data = (x_train, y_train, x_test)\n",
    "fp = open('../DataSets/DaGuan/TF-IDF.pkl', 'wb')\n",
    "pickle.dump(data, fp)\n",
    "fp.close()\n",
    "\n",
    "t_end = time.time()\n",
    "print(\"已将原始数据数字化为tfidf特征，共耗时：{}min\".format((t_end-t_start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1951', '1952', 'an', 'and', 'by', 'cuba', 'cuban', 'famous', 'far', 'fighting', 'fisherman', 'from', 'gulf', 'hemingway', 'huge', 'in', 'is', 'it', 'man', 'marlin', 'most', 'novella', 'of', 'old', 'one', 'published', 'sea', 'shore', 'story', 'stream', 'tells', 'the', 'was', 'works', 'written']\n",
      "(4, 35)\n"
     ]
    }
   ],
   "source": [
    "# 练习：\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'The Old Man and the Sea, a novella written by Hemingway in Cuba in 1951.',\n",
    "    'It was published in 1952.',\n",
    "    \"It is one of Hemingway's most famous works. \",\n",
    "    'It tells the story of an old Cuban fisherman fighting a huge Marlin in the Gulf Stream far from shore. ',\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    " \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备数据................ \n",
      "准备数据完成! \n",
      "训练完成! \n",
      " 保存训练结果........... \n",
      "训练结果已保存到该目录下！ \n",
      "耗时：4943.745208501816s \n"
     ]
    }
   ],
   "source": [
    "# 第三部分：word2vec\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import csv,sys\n",
    "vector_size = 100\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10\n",
    "    # as long as the OverflowError occurs.\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True\n",
    "\n",
    "\n",
    "# 0 辅助函数\n",
    "def sentence2list(sentence):\n",
    "    return sentence.strip().split()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data_path = '../DataSets/DaGuan/'\n",
    "feature_path = '../DataSets/DaGuan/feature_file/'\n",
    "proba_path = '../DataSets/DaGuan/proba_file/'\n",
    "model_path = '../DataSets/DaGuan/model_file/'\n",
    "result_path ='../DataSets/DaGuan/result/'\n",
    "\n",
    "\n",
    "# 1 准备训练数据\n",
    "print(\"准备数据................ \")\n",
    "df_train = pd.read_csv('../DataSets/DaGuan/train_set.csv')\n",
    "df_test = pd.read_csv('../DataSets/DaGuan/test_set.csv')\n",
    "\n",
    "sentences_train = list(df_train.loc[:, 'word_seg'].apply(sentence2list))\n",
    "sentences_test = list(df_test.loc[:, 'word_seg'].apply(sentence2list))\n",
    "sentences = sentences_train + sentences_test\n",
    "print(\"准备数据完成! \")\n",
    "\n",
    "# 2 训练\n",
    "model = gensim.models.Word2Vec(sentences=sentences, size=vector_size, window=5, min_count=5, workers=8, sg=0, iter=5)\n",
    "print(\"训练完成! \")\n",
    "\n",
    "\n",
    "# 3 提取词汇表及vectors,并保存\n",
    "print(\" 保存训练结果........... \")\n",
    "wv = model.wv\n",
    "vocab_list = wv.index2word\n",
    "word_idx_dict = {}\n",
    "for idx, word in enumerate(vocab_list):\n",
    "    word_idx_dict[word] = idx\n",
    "    \n",
    "vectors_arr = wv.vectors\n",
    "vectors_arr = np.concatenate((np.zeros(vector_size)[np.newaxis, :], vectors_arr), axis=0)#第0位置的vector为'unk'的vector\n",
    "\n",
    "f_wordidx = open(feature_path + 'word_seg_word_idx_dict.pkl', 'wb')\n",
    "f_vectors = open(feature_path + 'word_seg_vectors_arr.pkl', 'wb')\n",
    "pickle.dump(word_idx_dict, f_wordidx)\n",
    "pickle.dump(vectors_arr, f_vectors)\n",
    "f_wordidx.close()\n",
    "f_vectors.close()\n",
    "print(\"训练结果已保存到该目录下！ \")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"耗时：{}s \".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 读取特征\n",
      "模型训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesse/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/jesse/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/jesse/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 保存模型\n",
      "预测结果\n",
      "保存结果\n",
      "共耗时：-22.51min\n"
     ]
    }
   ],
   "source": [
    "# 第四部分：逻辑回归模型\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import  train_test_split\n",
    "time_start = time.time()\n",
    "\n",
    "data_path = '../DataSets/DaGuan/'\n",
    "feature_path = '../DataSets/DaGuan/feature_file/'\n",
    "proba_path = '../DataSets/DaGuan/proba_file/'\n",
    "model_path = '../DataSets/DaGuan/model_file/'\n",
    "result_path =\"../DataSets/DaGuan/result/\"\n",
    "\n",
    "\n",
    "#0 读取特征\n",
    "print(\"0 读取特征\")\n",
    "data_fp = open(feature_path + \"data_w_tfidf.pkl\", 'rb')\n",
    "x_train, y_train, x_test = pickle.load(data_fp)\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x_train, y_train, test_size=0.30, random_state=531)\n",
    "\n",
    "\n",
    "#1 模型训练\n",
    "print(\"模型训练\")\n",
    "lr = LogisticRegression(C=120,dual=True)\n",
    "lr.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "#2 保存模型\n",
    "\n",
    "print('2 保存模型')\n",
    "joblib.dump(lr, model_path + \"LR(120)_data_w_tfidf.m\")\n",
    "\n",
    "\n",
    "#3 预测结果 \n",
    "print(\"预测结果\")\n",
    "y_test = lr.predict(x_test)\n",
    "\n",
    "\n",
    "#4 保存结果 \n",
    "print(\"保存结果\")\n",
    "y_test = [i+1 for i in list(y_test)]\n",
    "df_result = pd.DataFrame({'id':range(len(y_test)),'class':y_test})\n",
    "\n",
    "df_result.to_csv(result_path + 'LR(c120)_data_w_tfidf.csv',index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "print('共耗时：{:.2f}min'.format((time_start-time_end)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 读取特征\n",
      "模型训练\n",
      "2 保存模型\n",
      "预测结果\n",
      "保存结果\n",
      "共耗时：-21.41min\n"
     ]
    }
   ],
   "source": [
    "# SVM：\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import  train_test_split\n",
    "time_start = time.time()\n",
    "\n",
    "data_path = '../DataSets/DaGuan/'\n",
    "feature_path = '../DataSets/DaGuan/feature_file/'\n",
    "proba_path = '../DataSets/DaGuan/proba_file/'\n",
    "model_path = '../DataSets/DaGuan/model_file/'\n",
    "result_path =\"../DataSets/DaGuan/result/\"\n",
    "\n",
    "\n",
    "#0 读取特征\n",
    "print(\"0 读取特征\")\n",
    "data_fp = open(feature_path  + \"data_w_tfidf.pkl\", 'rb')\n",
    "x_train, y_train, x_test = pickle.load(data_fp)\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x_train, y_train, test_size=0.30, random_state=531)\n",
    "\n",
    "\n",
    "#1 模型训练\n",
    "print(\"模型训练\")\n",
    "# clf = joblib.load('linearsvm_model_Tfid.1.m')\n",
    "clf = svm.LinearSVC(C=5,dual=False)\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "#2 保存模型\n",
    "print('2 保存模型')\n",
    "joblib.dump(clf, model_path + \"SVM(c5)_data_w_tfidf.m\")\n",
    "\n",
    "\n",
    "#3 预测结果 \n",
    "\n",
    "print(\"预测结果\")\n",
    "y_test = clf.predict(x_test)\n",
    "\n",
    "\n",
    "#4 保存结果 \n",
    "print(\"保存结果\")\n",
    "y_test = [i+1 for i in list(y_test)]\n",
    "df_result = pd.DataFrame({'id':range(len(y_test)),'class':y_test})\n",
    "df_result.to_csv(result_path + 'SVM(c5)_data_w_tfidf.csv',index=False)\n",
    "\n",
    "time_end = time.time()\n",
    "print('共耗时：{:.2f}min'.format((time_start-time_end)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 自定义验证集的评价函数\n",
      "1 读取数据,并转换到LGB的标准数据格式\n",
      "划分训练集和验证集，验证集比例为test_size\n",
      "3 训练LGB分类器\n",
      "[1]\tvalid_0's multi_logloss: 2.18267\tvalid_0's f1_score: 0.587845\n",
      "[2]\tvalid_0's multi_logloss: 1.9783\tvalid_0's f1_score: 0.624412\n",
      "[3]\tvalid_0's multi_logloss: 1.83931\tvalid_0's f1_score: 0.643124\n",
      "[4]\tvalid_0's multi_logloss: 1.73331\tvalid_0's f1_score: 0.657061\n",
      "[5]\tvalid_0's multi_logloss: 1.65183\tvalid_0's f1_score: 0.661236\n"
     ]
    }
   ],
   "source": [
    "# 第五部分：lightGBM模型： 24小时才训练了21次，设定800次，至少要在电脑上面运行一个月。\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import pickle\n",
    "import lightgbm as LGB\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "t_start = time.time()\n",
    "data_path = '../DataSets/DaGuan/'\n",
    "feature_path = '../DataSets/DaGuan/feature_file/'\n",
    "proba_path = '../DataSets/DaGuan/proba_file/'\n",
    "model_path = '../DataSets/DaGuan/model_file/'\n",
    "result_path =\"../DataSets/DaGuan/result/\"\n",
    "\n",
    "\n",
    "# 0 自定义验证集的评价函数\n",
    "print(\"0 自定义验证集的评价函数\")\n",
    "def f1_score_vali(preds, data_vali):\n",
    "    labels = data_vali.get_label()\n",
    "    preds = np.argmax(preds.reshape(19, -1), axis=0)\n",
    "    score_vali = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    return 'f1_score', score_vali, True\n",
    "\n",
    "\n",
    "#1 读取数据,并转换到LGB的标准数据格式\n",
    "print(\"1 读取数据,并转换到LGB的标准数据格式\")\n",
    "data_fp = open(feature_path + 'data_w_tfidf.pkl' , 'rb')\n",
    "x_train, y_train, x_test = pickle.load(data_fp)\n",
    "data_fp.close()\n",
    "\n",
    "#2 划分训练集和验证集，验证集比例为test_size\n",
    "\n",
    "print(\"划分训练集和验证集，验证集比例为test_size\")\n",
    "x_train, x_vali, y_train, y_vali = train_test_split(x_train, y_train, test_size=0.1, random_state=0)\n",
    "d_train = LGB.Dataset(data=x_train, label=y_train)\n",
    "d_vali = LGB.Dataset(data=x_vali, label=y_vali)\n",
    "\n",
    "\n",
    "#3 训练LGB分类器\n",
    "print(\"3 训练LGB分类器\")\n",
    "params = {\n",
    "    'boosting': 'gbdt',\n",
    "    'application': 'multiclassova',\n",
    "    'num_class': 19,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'lambda_l1': 0,\n",
    "    'lambda_l2': 0.5,\n",
    "    'bagging_fraction': 1.0,\n",
    "\n",
    "}\n",
    "\n",
    "bst = LGB.train(params, d_train, num_boost_round=10, valid_sets=d_vali, feval=f1_score_vali,\n",
    "                early_stopping_rounds=None,\n",
    "                verbose_eval=True)                   # num_boost_round=800 ： 迭代800次，这里由于笔记本上跑，点到为止\n",
    "\n",
    "joblib.dump(bst, model_path + \"LGB_data_w_tfidf.m\")\n",
    "\n",
    "\n",
    "#4 对测试集进行预测;将预测结果转换为官方标准格式；并将结果保存至本地\n",
    "print(\"4 对测试集进行预测;将预测结果转换为官方标准格式；并将结果保存至本地\")\n",
    "y_proba = bst.predict(x_test)\n",
    "y_test = np.argmax(y_proba, axis=1) + 1\n",
    "\n",
    "df_result = pd.DataFrame(data={'id': range(5000), 'class': y_test.tolist()})\n",
    "df_proba = pd.DataFrame(data={'id': range(5000), 'proba': y_proba.tolist()})\n",
    "\n",
    "df_result.to_csv(result_path  + 'LGB_data_w_tfidf_result.csv', index=False)\n",
    "df_proba.to_csv(result_path + 'LGB_data_w_tfidf_proba.csv', index=False)\n",
    "t_end = time.time()\n",
    "print(\"训练结束，耗时:{}min\".format((t_end - t_start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
