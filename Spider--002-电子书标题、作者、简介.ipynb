{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.allitebooks.com/page/1\n",
      "http://www.allitebooks.com/page/2\n",
      "http://www.allitebooks.com/page/3\n",
      "http://www.allitebooks.com/page/4\n",
      "http://www.allitebooks.com/page/5\n",
      "http://www.allitebooks.com/page/6\n",
      "http://www.allitebooks.com/page/7\n",
      "http://www.allitebooks.com/page/8\n",
      "http://www.allitebooks.com/page/9\n",
      "http://www.allitebooks.com/page/10\n",
      "cost time:  53.03641057014465 s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "本文以'allitebooks'网站对象，实现电子书标题、作者、简介批量获取，并以json和csv文件形式存入本地。\n",
    "代码使用python的requests模块和xpath、bs4两种方式，并以json和csv格式转存本地。\n",
    "分成4步：1,发请求；2,解析数据；3,保存数据；4，json转换成csv。\n",
    "Code：1，使用xpath；2，使用bs4。'''\n",
    "\n",
    "# 1，使用xpath：\n",
    "import requests\n",
    "from lxml import etree\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "\n",
    "class BookSpider(object):\n",
    "    def __init__(self):\n",
    "        self.base_url = 'http://www.allitebooks.com/page/{}'\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/73.0.3683.86 Chrome/73.0.3683.86 Safari/537.36'}\n",
    "        self.data_of_book_dict = []\n",
    "\n",
    "\n",
    "    # 1,构建所有的url：\n",
    "    def get_url_list(self):\n",
    "        url_list = []\n",
    "        for i in range(1, 11):  # 爬取前10页,电子书标题、作者、简介\n",
    "            url = self.base_url.format(i)\n",
    "            url_list.append(url)\n",
    "        return url_list\n",
    "\n",
    "    # 2,发请求：\n",
    "    def send_request(self, url):\n",
    "        data = requests.get(url, headers = self.headers).content.decode()\n",
    "        return data\n",
    "\n",
    "    # 3,解析数据：\n",
    "    def parse_xpath_data(self, data):\n",
    "        parse_data = etree.HTML(data)\n",
    "        # 1,解析出所有的书：\n",
    "        book_list = parse_data.xpath('//div[@class=\"main-content-inner clearfix\"]/article')\n",
    "        # print(len(book_list))\n",
    "\n",
    "        # 2,解析出每本书的信息：\n",
    "        for book in book_list:\n",
    "            book_dict = {}\n",
    "            # 1,书名：\n",
    "            book_dict['book_name'] = book.xpath('.//h2[@class=\"entry-title\"]//text()')  # .:表示当前路径, //：表示跨节点\n",
    "            #print(book_name)\n",
    "\n",
    "            # 2,该书的url：获取该路径下面的属性src\n",
    "            book_dict['book_img_url'] = book.xpath('./div[@class=\"entry-thumbnail hover-thumb\"]/a/img/@src')[0]  # book_list中元素的xpath路径article(相当于本地路径)下面的标签div\n",
    "            #print(book_img_url)\n",
    "\n",
    "            # 3,书的作者：\n",
    "            book_dict['book_author'] = book.xpath('.//h5[@class=\"entry-author\"]/a/text()')[0]\n",
    "            #print(book_author)\n",
    "\n",
    "            # 4,书的简介：\n",
    "            book_dict['book_info'] = book.xpath('.//div[@class=\"entry-summary\"]/p/text()')[0]  # 也可以写//text()替换/p/text()\n",
    "            #print(book_info)\n",
    "            self.data_of_book_dict.append(book_dict)\n",
    "\n",
    "\n",
    "    # 4,保存数据：\n",
    "    def save_data(self):\n",
    "        json.dump(self.data_of_book_dict, open('../OutPut/ebook_xpath.json', 'w'))\n",
    "\n",
    "    # 5,启动：\n",
    "    def start(self):\n",
    "        url_list = self.get_url_list()\n",
    "\n",
    "        # 循环遍历发送请求：\n",
    "        for url in url_list:\n",
    "            print(url)\n",
    "            data = self.send_request(url)\n",
    "            self.parse_xpath_data(data)\n",
    "\n",
    "        self.save_data()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "BookSpider().start()\n",
    "\n",
    "# 将json数据转换成csv格式：列表数据转csv\n",
    "# 1,读 创建文件：\n",
    "json_fp = open('../OutPut/ebook_xpath.json', 'r')\n",
    "csv_fp = open('../OutPut/ebook_xpath.csv', 'w')\n",
    "\n",
    "# 2,提出表头 表内容：\n",
    "data_list = json.load(json_fp)\n",
    "sheet_title = data_list[0].keys()  # 方法一\n",
    "\n",
    "\n",
    "sheet_data = []\n",
    "for data in data_list:\n",
    "    sheet_data.append(data.values())\n",
    "# print(sheet_data)\n",
    "\n",
    "# 3,csv写入器：\n",
    "writer = csv.writer(csv_fp)\n",
    "\n",
    "# 4,写入表头：\n",
    "writer.writerow(sheet_title)\n",
    "\n",
    "# 5,写入内容：\n",
    "writer.writerows(sheet_data)\n",
    "\n",
    "# 6,关闭两个文件：\n",
    "json_fp.close()\n",
    "csv_fp.close()\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('cost time: ', end-start, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.allitebooks.com/page/1\n",
      "http://www.allitebooks.com/page/2\n",
      "http://www.allitebooks.com/page/3\n",
      "http://www.allitebooks.com/page/4\n",
      "http://www.allitebooks.com/page/5\n",
      "http://www.allitebooks.com/page/6\n",
      "http://www.allitebooks.com/page/7\n",
      "http://www.allitebooks.com/page/8\n",
      "http://www.allitebooks.com/page/9\n",
      "http://www.allitebooks.com/page/10\n",
      "cost time:  44.27156138420105 s\n"
     ]
    }
   ],
   "source": [
    "# 2，使用bs4：\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "\n",
    "class BookSpider(object):\n",
    "    def __init__(self):\n",
    "        self.base_url = 'http://www.allitebooks.com/page/{}'\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/73.0.3683.86 Chrome/73.0.3683.86 Safari/537.36'}\n",
    "        self.data_of_book_dict = []\n",
    "\n",
    "\n",
    "    # 1,构建所有的url：\n",
    "    def get_url_list(self):\n",
    "        url_list = []\n",
    "        for i in range(1, 11):  # 爬取前10页,电子书标题、作者、简介\n",
    "            url = self.base_url.format(i)\n",
    "            url_list.append(url)\n",
    "        return url_list\n",
    "\n",
    "    # 2,发请求：\n",
    "    def send_request(self, url):\n",
    "        data = requests.get(url, headers = self.headers).content.decode()\n",
    "        return data\n",
    "\n",
    "    # 3,解析数据：\n",
    "    def parse_bs4_data(self, data):\n",
    "        bs4_data = BeautifulSoup(data, 'lxml')\n",
    "\n",
    "        # 1,解析出所有的书：\n",
    "        book_list = bs4_data.select('article')\n",
    "        #print(len(book_list))\n",
    "\n",
    "        # 2,解析出每本书的信息：\n",
    "        for book in book_list:\n",
    "            book_dict = {}\n",
    "            # 1,书名：\n",
    "            book_dict['book_name'] = book.select_one('.entry-title').get_text()\n",
    "\n",
    "\n",
    "            # 2,该书的url：获取该路径下面的属性src\n",
    "            book_dict['book_img_url'] = book.select_one('.attachment-post-thumbnail').get('src')\n",
    "\n",
    "\n",
    "            # 3,书的作者：\n",
    "            book_dict['book_author'] = book.select_one('.entry-author').get_text()[3:]\n",
    "\n",
    "\n",
    "            # 4,书的简介：\n",
    "            book_dict['book_info'] = book.select_one('.entry-summary').get_text()\n",
    "\n",
    "            self.data_of_book_dict.append(book_dict)\n",
    "            #print(book_dict)\n",
    "\n",
    "\n",
    "\n",
    "    # 4,保存数据：\n",
    "    def save_data(self):\n",
    "        json.dump(self.data_of_book_dict, open('../OutPut/ebook_bs4.json', 'w'))\n",
    "\n",
    "    # 5,启动：\n",
    "    def start(self):\n",
    "        url_list = self.get_url_list()\n",
    "\n",
    "        # 循环遍历发送请求：\n",
    "        for url in url_list:\n",
    "            print(url)\n",
    "            data = self.send_request(url)\n",
    "            self.parse_bs4_data(data)\n",
    "\n",
    "        self.save_data()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "BookSpider().start()\n",
    "\n",
    "\n",
    "# 将json数据转换成csv格式：列表数据转csv\n",
    "# 1,读 创建文件：\n",
    "json_fp = open('../OutPut/ebook_bs4.json', 'r')\n",
    "csv_fp = open('../OutPut/ebook_bs4.csv', 'w')\n",
    "\n",
    "# 2,提出表头 表内容：\n",
    "data_list = json.load(json_fp)\n",
    "sheet_title = data_list[0].keys()  # 方法一\n",
    "\n",
    "\n",
    "sheet_data = []\n",
    "for data in data_list:\n",
    "    sheet_data.append(data.values())\n",
    "# print(sheet_data)\n",
    "\n",
    "# 3,csv写入器：\n",
    "writer = csv.writer(csv_fp)\n",
    "\n",
    "# 4,写入表头：\n",
    "writer.writerow(sheet_title)\n",
    "\n",
    "# 5,写入内容：\n",
    "writer.writerows(sheet_data)\n",
    "\n",
    "# 6,关闭两个文件：\n",
    "json_fp.close()\n",
    "csv_fp.close()\n",
    "\n",
    "end = time.time()\n",
    "print('cost time: ', end-start, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
